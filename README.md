# Agentic RAG - Production-Grade AI System

A **production-grade Agentic Retrieval-Augmented Generation (RAG)** system implementing the 12-step workflow with LangChain and LangGraph.

## ğŸŒŸ Overview

This is a complete, production-ready RAG system with specialized AI agents at each stage. The system uses **intelligent decision-making** at every step, resulting in higher quality responses than traditional RAG systems.

### ğŸ¯ What Makes It "Agentic"?

Unlike traditional RAG that follows a fixed pipeline, this system:
- **Thinks**: Each agent actively analyzes and makes decisions
- **Adapts**: Routes differently based on query needs  
- **Evaluates**: Self-assesses quality and retries if needed
- **Learns**: Iteratively improves responses

## ğŸ—ï¸ Architecture

The system follows a 12-step workflow with multiple decision points:

1. **Query Rewriting** - Optimize user queries for better retrieval
2. **Information Need Assessment** - Decide if external data is needed
3. **Source Selection** - Choose the best information source
4. **Context Retrieval** - Fetch relevant information
5. **Answer Generation** - Create comprehensive responses
6. **Answer Evaluation** - Validate response quality
7. **Iterative Refinement** - Retry if quality is insufficient

### Workflow Diagram

```
START â†’ Query Rewriter â†’ Needs More Info? 
           â†“                    â†“
       (No Info)          Source Selector
           â†“                    â†“
    Answer Generator â† Retriever
           â†“
    Answer Evaluator
           â†“
    Relevant? â†’ YES â†’ END
           â†“
          NO (Max retries?) â†’ Retry from Start
```

## ğŸ“ Project Structure

```
AGENTIC-RAG/
â”‚
â”œâ”€â”€ src/                            # Application source code
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/                     # All agent logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ query_rewriter.py       # Steps 1â€“2
â”‚   â”‚   â”œâ”€â”€ needs_more_info.py      # Steps 3â€“4
â”‚   â”‚   â”œâ”€â”€ source_selector.py      # Steps 5â€“6
â”‚   â”‚   â”œâ”€â”€ answer_generator.py     # Steps 8â€“9
â”‚   â”‚   â””â”€â”€ answer_evaluator.py     # Steps 10â€“12
â”‚   â”‚
â”‚   â”œâ”€â”€ graph/                      # LangGraph workflow
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ state.py                # Shared graph state
â”‚   â”‚   â””â”€â”€ rag_graph.py            # Agentic RAG graph
â”‚   â”‚
â”‚   â”œâ”€â”€ retrieval/                  # Knowledge access layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ vector_store.py         # FAISS / Chroma / Pinecone
â”‚   â”‚   â”œâ”€â”€ tools.py                # External tools & APIs
â”‚   â”‚   â””â”€â”€ web_search.py           # Internet search
â”‚   â”‚
â”‚   â”œâ”€â”€ llms/                       # LLM configuration
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ model.py                # ChatOpenAI, etc.
â”‚   â”‚
â”‚   â”œâ”€â”€ prompts/                    # Prompt templates
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ rewrite_prompt.py
â”‚   â”‚   â”œâ”€â”€ retrieval_prompt.py
â”‚   â”‚   â””â”€â”€ evaluation_prompt.py
â”‚   â”‚
â”‚   â”œâ”€â”€ config/                     # App configuration
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ settings.py             # Env vars, model names
â”‚   â”‚   â””â”€â”€ constants.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/                      # Shared utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ logging.py
â”‚
â”œâ”€â”€ data/                           # Local data
â”‚   â”œâ”€â”€ documents/                  # Source documents
â”‚   â””â”€â”€ embeddings/                 # Vector embeddings
â”‚
â”œâ”€â”€ tests/                          # Tests
â”‚   â”œâ”€â”€ test_agents.py
â”‚   â””â”€â”€ test_graph.py
â”‚
â”œâ”€â”€ main.py                         # Entry point
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â””â”€â”€ LICENSE
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- Groq API key (free) or OpenAI API key
- (Optional) Tavily API key for web search

### Installation

1. **Clone the repository**
   ```bash
   git clone <your-repo-url>
   cd Agentic-Rag
   ```

2. **Create virtual environment**
   ```bash
   python -m venv .venv
   
   # Windows
   .venv\Scripts\activate
   
   # Linux/Mac
   source .venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   # Using pip
   pip install -r requirements.txt
   
   # Or using uv (faster)
   uv pip install -r requirements.txt
   ```

4. **Set up environment variables**
   ```bash
   cp .env.example .env
   ```
   
   Edit `.env` and configure:
   ```env
   # LLM Configuration (using Groq - FREE!)
   GROQ_API_KEY=your_groq_api_key_here
   LLM_MODEL=llama-3.3-70b-versatile
   LLM_TEMPERATURE=0.4
   
   # Vector Store (uses free local embeddings)
   VECTOR_STORE_TYPE=chroma
   CHROMA_PERSIST_DIR=./data/chroma
   
   # Optional: Web Search
   TAVILY_API_KEY=your_tavily_key_here
   ```

5. **Verify setup**
   ```bash
   python setup.py
   ```

### Usage

#### Interactive Mode

```bash
python main.py
```

This starts an interactive session where you can:
- Ask questions continuously
- Type `index` to index documents from `data/documents/`
- Type `quit` to exit

#### Single Query Mode

```bash
python main.py --query "What is machine learning?"
```

#### Index Documents First

```bash
python main.py --index --query "Tell me about the documents"
```

## ğŸ“š Adding Your Documents

1. Place your text documents in `data/documents/`
2. Run the indexing command:
   ```bash
   python main.py --index
   ```
   Or type `index` in interactive mode

**Incremental Indexing** (Production Feature):
- System tracks which files are already indexed
- Only processes NEW or MODIFIED documents
- Much faster when adding documents incrementally
- Metadata stored in `data/chroma/indexed_files.json`

Supported formats: `.txt` (more formats can be added)

## ğŸ”§ Configuration
 (using Groq - FREE)
GROQ_API_KEY=your_groq_api_key
LLM_MODEL=llama-3.3-70b-versatile
LLM_TEMPERATURE=0.4

# Vector Store (uses free local HuggingFace embeddings)
VECTOR_STORE_TYPE=chroma
CHROMA_PERSIST_DIR=./data/chroma
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# RAG Parameters
MAX_ITERATIONS=3
RETRIEVAL_TOP_K=5
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# Logging (saves to logs/agentic_rag_TIMESTAMP.log)
LOG_LEVEL=INFO

# Optional: Web Search
TAVILY_API_KEY=your_tavily_key
SERPAPI_API_KEY=your_serpapi
RETRIEVAL_TOP_K=5
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# Optional: Web Search
TAVILY_API_KEY=your_tavily_key
```

### Customizing Agents

Each agent can be customized by modifying its corresponding file in `src/agents/`:

- **Query Rewriter**: Adjust rewriting strategy in `src/agents/query_rewriter.py`
- **Source Selector**: Modify source selection logic in `src/agents/source_selector.py`
- **Answer Evaluator**: Change evaluation criteria in `src/agents/answer_evaluator.py`

### Modifying Prompts

All prompts are in `src/prompts/`. Edit them to change agent behavior:

- `src/prompts/rewrite_prompt.py`
- `src/prompts/retrieval_prompt.py`
- `src/prompts/evaluation_prompt.py`

## ğŸ§ª Testing

Run tests:

```bash
pytest tests/
```

Run with coverage:

```bash
pytest tests/ --cov=src --cov-report=html
```

## ğŸ¯ Key Features

### âœ¨ Agentic Behaviors

- **Intelligent Query Rewriting**: Automatically improves queries for better retrieval
- **Conditional Routing**: Different paths based on query analysis

### ğŸ” Multiple Information Sources

- **Vector Database**: ChromaDB for document retrieval (with incremental indexing)
- **External Tools/APIs**: Calculator, datetime, and custom tools
- **Web Search**: Real-time internet information (Tavily/SerpAPI)

### ğŸ“Š Production Features

- **FREE to Use**: Uses Groq API (free) + local embeddings (no OpenAI costs)
- **Incremental Indexing**: Only processes new/modified documents
- **Timestamped Logging**: Each run creates separate log file in `logs/`
- **Absolute Imports**: Clean, direct imports (no relative imports)
- **Structured Logging**: Comprehensive logging for debugging
- **Configurable Parameters**: Easy customization via environment variables
- **Error Handling**: Graceful degradation and informative errors
- **Modular Design**: Easy to extend and maintain
- **Type Hints**: Full type annotations throughout
- **Comprehensive Documentation**: Docstrings in every moduleor debugging
- **Configurable Parameters**: Easy customization via environment variables
- **Error Handling**: Graceful degradation and informative errors
- **Modular Design**: Easy to extend and maintain

## ğŸ› ï¸ Advanced Usage

### Adding Custom Tools

Edit `src/retrieval/tools.py`:

```python
def _my_custom_tool(self, query: str) -> str:
    """Your custom tool logic."""
    return "Tool result"

# Register in __init__
self.available_tools["my_tool"] = self._my_custom_tool
```

### Using Different Vector Stores

Configure in `.env`:

```env
VECTOR_STORE_TYPE=faiss  # or pinecone
### Complete 12-Step Workflow:

1. **Query Rewriting (Steps 1-2)**: Optimizes user query for better retrieval
2. **Needs More Info (Steps 3-4)**: Decides if retrieval is needed or direct answer
3. **Source Selection (Steps 5-6)**: Chooses best source (vector DB, tools, web)
4. **Context Retrieval (Step 7)**: Fetches relevant information
5. **Answer Generation (Steps 8-9)**: Creates response with context
6. **Answer Evaluation (Steps 10-12)**: Validates quality, retries if needed

### Intelligent Routing:

```
Query â†’ Rewrite â†’ Need Info?
                      â”œâ”€ NO â†’ Direct Answer â†’ END
                      â””â”€ YES â†’ Select Source â†’ Retrieve â†’ Generate â†’ Evaluate
                                                                         â”œâ”€ GOOD â†’ END
                                                                         â””â”€ BAD â†’ RETRY (max 3)
```

### Example Queries:

**Simple Math (No Retrieval)**:
```bash
python main.py -q "What is 2 + 2?"
# Ouï¿½ Technologies Used

- **LangChain**: LLM orchestration framework
- **LangGraph**: State machine for agent workflows  
- **Groq**: Fast, free LLM API (Llama 3.3 70B)
- **ChromaDB**: Vector database for embeddings
- **HuggingFace**: Free local embeddings (all-MiniLM-L6-v2)
- **Pydantic**: Configuration and validation
- **Python-dotenv**: Environment management
- **Pytest**: Testing framework

## âœ… Production Checklist

- [x] Complete 12-step agentic workflow
- [x] 5 specialized agents implemented
- [x] 3 retrieval sources (vector DB, tools, web)
- [x] LangGraph state management with conditional routing
- [x] FREE to use (Groq + local embeddings)
- [x] Incremental document indexing
- [x] Timestamped logging for each run
- [x] Absolute imports (no relative imports)
- [x] Comprehensive error handling
- [x] CLI interface (interactive + single query)
- [x] Environment-based configuration
- [x] Full type hints and docstrings
- [x] Test suite included
- [x] Sample documents provided

## ğŸš§ Future Enhancements

Potential improvements:
- [ ] Add streaming responses
- [ ] Implement conversation memory
- [ ] Support PDF and DOCX documents
- [ ] Add FastAPI REST API
- [ ] Implement re-ranking for better retrieval
- [ ] Add monitoring dashboard
- [ ] Support more LLM providers
- [ ] Multi-language support

## ğŸ™ Acknowledgments

- Built with [LangChain](https://langchain.com/) and [LangGraph](https://langchain-ai.github.io/langgraph/)
- Powered by [Groq](https://groq.com/) for fast, free LLM inference
**Document Query (Vector DB)**:
```bash
python main.py -q "What is machine learning?"
# Uses: vector_database â†’ retrieves from indexed documents
```

**Real-time Info (Web Search)**:
```bash
python main.py -q "Who is the current PM of India?"
# Uses: web_search â†’ fetches current information
```

1. **User submits a query** â†’ System initializes the workflow
2. **Query Rewriter Agent** â†’ Optimizes the query for retrieval
3. **Needs Info Agent** â†’ Decides if external information is required
4. **If Yes** â†’ Source Selector Agent chooses best source (DB/API/Web)
5. **Retriever** â†’ Fetches relevant context from selected source
6. **Answer Generator** â†’ Creates response using context
7. **Answer Evaluator** â†’ Checks quality and relevance
8. **If Poor Quality** â†’ Retry from step 2 (up to MAX_ITERATIONS)
9. **Return Final Answer** â†’ User receives the response

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“ License

See LICENSE file for details.

## ğŸ™ Acknowledgments

- Built with [LangChain](https://langchain.com/) and [LangGraph](https://langchain-ai.github.io/langgraph/)
- Inspired by advanced RAG architectures and agentic AI systems

## ğŸ“§ Contact

For questions or support, please open an issue on GitHub.

---

**Happy Building! ğŸš€**